python train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-high/checkpoints --encoder-dropout-in 0.5 --encoder-dropout-out 0.5 --decoder-dropout-in 0.5 --decoder-dropout-out 0.5 --cuda 1

python train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-high/checkpoints --encoder-dropout-in 0.5 --encoder-dropout-out 0.5 --decoder-dropout-in 0.5 --decoder-dropout-out 0.5 --cuda 1
INFO: Commencing training!
INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-high/checkpoints --encoder-dropout-in 0.5 --encoder-dropout-out 0.5 --decoder-dropout-in 0.5 --decoder-dropout-out 0.5 --cuda 1
INFO: Arguments: {'cuda': '1', 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/dropout-high/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.5, 'encoder_dropout_out': 0.5, 'decoder_dropout_in': 0.5, 'decoder_dropout_out': 0.5, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.752 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.01 | clip 1
INFO: Epoch 000: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 145
INFO: Epoch 001: loss 4.268 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.49 | clip 1
INFO: Epoch 001: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.9
INFO: Epoch 002: loss 4.058 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.23 | clip 1
INFO: Epoch 002: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80
INFO: Epoch 003: loss 3.91 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.75 | clip 1
INFO: Epoch 003: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70
INFO: Epoch 004: loss 3.82 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.25 | clip 1
INFO: Epoch 004: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.7
INFO: Epoch 005: loss 3.741 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.57 | clip 1
INFO: Epoch 005: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.1
INFO: Epoch 006: loss 3.688 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.1 | clip 1
INFO: Epoch 006: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.4
INFO: Epoch 007: loss 3.632 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.22 | clip 1
INFO: Epoch 007: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4
INFO: Epoch 008: loss 3.582 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.95 | clip 1
INFO: Epoch 008: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.2
INFO: Epoch 009: loss 3.545 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.06 | clip 1
INFO: Epoch 009: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.7
INFO: Epoch 010: loss 3.499 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.45 | clip 1
INFO: Epoch 010: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7
INFO: Epoch 011: loss 3.464 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.18 | clip 1
INFO: Epoch 011: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41
INFO: Epoch 012: loss 3.428 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.74 | clip 1
INFO: Epoch 012: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.4
INFO: Epoch 013: loss 3.392 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.84 | clip 1
INFO: Epoch 013: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.9
INFO: Epoch 014: loss 3.363 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.06 | clip 1
INFO: Epoch 014: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8
INFO: Epoch 015: loss 3.326 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.85 | clip 1
INFO: Epoch 015: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.9
INFO: Epoch 016: loss 3.3 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.49 | clip 1
INFO: Epoch 016: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6
INFO: Epoch 017: loss 3.262 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.14 | clip 1
INFO: Epoch 017: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9
INFO: Epoch 018: loss 3.244 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.14 | clip 1
INFO: Epoch 018: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9
INFO: Epoch 019: loss 3.214 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.85 | clip 1
INFO: Epoch 019: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3
INFO: Epoch 020: loss 3.191 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.42 | clip 1
INFO: Epoch 020: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4
INFO: Epoch 021: loss 3.17 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.43 | clip 1
INFO: Epoch 021: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8
INFO: Epoch 022: loss 3.15 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.11 | clip 1
INFO: Epoch 022: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.6
INFO: Epoch 023: loss 3.132 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.8 | clip 1
INFO: Epoch 023: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.6
INFO: Epoch 024: loss 3.117 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.63 | clip 1
INFO: Epoch 024: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4
INFO: Epoch 025: loss 3.098 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.51 | clip 1
INFO: Epoch 025: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9
INFO: Epoch 026: loss 3.094 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.45 | clip 1
INFO: Epoch 026: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.7
INFO: Epoch 027: loss 3.086 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.41 | clip 1
INFO: Epoch 027: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1
INFO: Epoch 028: loss 3.064 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.38 | clip 1
INFO: Epoch 028: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8
INFO: Epoch 029: loss 3.056 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.54 | clip 1
INFO: Epoch 029: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2
INFO: Epoch 030: loss 3.047 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.79 | clip 1
INFO: Epoch 030: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2
INFO: Epoch 031: loss 3.038 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.02 | clip 1
INFO: Epoch 031: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7
INFO: Epoch 032: loss 3.024 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.72 | clip 1
INFO: Epoch 032: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23
INFO: Epoch 033: loss 3.015 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.79 | clip 1
INFO: Epoch 033: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23
INFO: Epoch 034: loss 3.008 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.99 | clip 1
INFO: Epoch 034: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1
INFO: Epoch 035: loss 2.996 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.75 | clip 1
INFO: Epoch 035: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9
INFO: Epoch 036: loss 2.989 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.14 | clip 1
INFO: Epoch 036: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 037: loss 2.978 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.3 | clip 1
INFO: Epoch 037: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4
INFO: Epoch 038: loss 2.973 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.09 | clip 1
INFO: Epoch 038: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21
INFO: Epoch 039: loss 2.965 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.65 | clip 1
INFO: Epoch 039: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7
INFO: Epoch 040: loss 2.955 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.47 | clip 1
INFO: Epoch 040: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4
INFO: Epoch 041: loss 2.947 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.39 | clip 1
INFO: Epoch 041: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4
INFO: Epoch 042: loss 2.945 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44 | clip 1
INFO: Epoch 042: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9
INFO: Epoch 043: loss 2.94 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.69 | clip 1
INFO: Epoch 043: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6
INFO: Epoch 044: loss 2.922 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.74 | clip 1
INFO: Epoch 044: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8
INFO: Epoch 045: loss 2.923 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.85 | clip 1
INFO: Epoch 045: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7
INFO: Epoch 046: loss 2.913 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.77 | clip 1
INFO: Epoch 046: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8
INFO: No validation set improvements observed for 3 epochs. Early stop!

python translate.py --data data/en-fr/prepared --dicts data/en-fr/prepared --data data/en-fr/prepared --checkpoint-path assignments/03/dropout-high/checkpoints/checkpoint_last.pt --output assignments/03/dropout-high/dropout_high_translations.txt

scripts/postprocess.sh assignments/03/dropout-high/dropout_high_translations.txt assignments/03/dropout-high/dropout_high_translations.p.txt en

cat assignments/03/dropout-high/dropout_high_translations.p.txt | sacrebleu data/en-fr/raw/test.en

{
 "name": "BLEU",
 "score": 6.9,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "30.4/10.5/4.2/1.7 (BP = 1.000 ratio = 1.617 hyp_len = 6293 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}


-------------------------------------------------------------------------------------------------------------------------------------------------------
python train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-low/checkpoints --encoder-dropout-in 0.1 --encoder-dropout-out 0.1 --decoder-dropout-in 0.1 --decoder-dropout-out 0.1 --cuda 1

python translate.py --data data/en-fr/prepared --dicts data/en-fr/prepared --data data/en-fr/prepared --checkpoint-path assignments/03/dropout-low/checkpoints/checkpoint_last.pt --output assignments/03/dropout-low/dropout_low_translations.txt
scripts/postprocess.sh assignments/03/dropout-low/dropout_low_translations.txt assignments/03/dropout-low/dropout_low_translations.p.txt en

cat assignments/03/dropout-low/dropout_low_translations.p.txt | sacrebleu data/en-fr/raw/test.en


python train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-low/checkpoints --encoder-dropout-in 0.1 --encoder-dropout-out 0.1 --decoder-dropout-in 0.1 --decoder-dropout-out 0.1 --cuda 1
INFO: Commencing training!
INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/dropout-low/checkpoints --encoder-dropout-in 0.1 --encoder-dropout-out 0.1 --decoder-dropout-in 0.1 --decoder-dropout-out 0.1 --cuda 1
INFO: Arguments: {'cuda': '1', 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/dropout-low/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.409 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 30.19 | clip 0.9979
INFO: Epoch 000: valid_loss 4.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 133
INFO: Epoch 001: loss 3.598 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 35.35 | clip 1
INFO: Epoch 001: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117
INFO: Epoch 002: loss 3.238 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.35 | clip 1
INFO: Epoch 002: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100
INFO: Epoch 003: loss 2.991 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.98 | clip 1
INFO: Epoch 003: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.3
INFO: Epoch 004: loss 2.793 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.63 | clip 1
INFO: Epoch 004: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.3
INFO: Epoch 005: loss 2.624 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.07 | clip 0.9998
INFO: Epoch 005: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.4
INFO: Epoch 006: loss 2.484 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.34 | clip 0.9998
INFO: Epoch 006: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.7
INFO: Epoch 007: loss 2.358 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.33 | clip 0.9998
INFO: Epoch 007: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6
INFO: Epoch 008: loss 2.25 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 53.52 | clip 0.9998
INFO: Epoch 008: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7
INFO: Epoch 009: loss 2.156 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 55.13 | clip 0.9992
INFO: Epoch 009: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1
INFO: Epoch 010: loss 2.068 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 54.92 | clip 0.9988
INFO: Epoch 010: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1
INFO: Epoch 011: loss 1.996 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 56.75 | clip 0.9986
INFO: Epoch 011: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9
INFO: Epoch 012: loss 1.925 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 57.2 | clip 0.9978
INFO: Epoch 012: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2
INFO: Epoch 013: loss 1.864 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 57.52 | clip 0.9968
INFO: Epoch 013: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3
INFO: Epoch 014: loss 1.813 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.3 | clip 0.9968
INFO: Epoch 014: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23
INFO: Epoch 015: loss 1.761 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.81 | clip 0.9959
INFO: Epoch 015: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21
INFO: Epoch 016: loss 1.721 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.13 | clip 0.9958
INFO: Epoch 016: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6
INFO: Epoch 017: loss 1.679 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.8 | clip 0.9946
INFO: Epoch 017: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5
INFO: Epoch 018: loss 1.64 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.99 | clip 0.9925
INFO: Epoch 018: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7
INFO: Epoch 019: loss 1.61 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.36 | clip 0.9922
INFO: Epoch 019: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8
INFO: Epoch 020: loss 1.576 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.82 | clip 0.9906
INFO: Epoch 020: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9
INFO: Epoch 021: loss 1.546 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.2 | clip 0.9897
INFO: Epoch 021: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8
INFO: Epoch 022: loss 1.522 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.57 | clip 0.9897
INFO: Epoch 022: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8
INFO: Epoch 023: loss 1.493 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.19 | clip 0.9895
INFO: Epoch 023: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 024: loss 1.464 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.1 | clip 0.9873
INFO: Epoch 024: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 025: loss 1.439 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.18 | clip 0.9864
INFO: Epoch 025: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7
INFO: Epoch 026: loss 1.418 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.37 | clip 0.9849
INFO: Epoch 026: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6
INFO: No validation set improvements observed for 3 epochs. Early stop!

{
 "name": "BLEU",
 "score": 14.8,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "44.3/20.3/10.3/5.2 (BP = 1.000 ratio = 1.290 hyp_len = 5021 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
